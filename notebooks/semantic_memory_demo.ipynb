{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "semantic-memory-title",
   "metadata": {},
   "source": [
    "# üß† Gianna Semantic Memory System - Complete Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive semantic memory system implemented for Gianna AI Assistant, including:\n",
    "\n",
    "- **Embedding-based storage** with multiple provider support\n",
    "- **Vector similarity search** using Chroma, FAISS, or in-memory stores\n",
    "- **Automatic clustering** of similar interactions\n",
    "- **Context summarization** and pattern detection\n",
    "- **Integration with GiannaState** management\n",
    "- **Tool integration** for LangGraph workflows\n",
    "\n",
    "## Features Demonstrated\n",
    "\n",
    "‚úÖ **Multi-provider embeddings** (OpenAI, SentenceTransformers, HuggingFace)\n",
    "‚úÖ **Multiple vector stores** (Chroma, FAISS, In-Memory)\n",
    "‚úÖ **Semantic similarity search** with configurable thresholds\n",
    "‚úÖ **Automatic clustering** of similar conversations\n",
    "‚úÖ **Pattern detection** and user preference analysis\n",
    "‚úÖ **Context summarization** with intelligent aggregation\n",
    "‚úÖ **Fallback strategies** when advanced providers aren't available\n",
    "‚úÖ **Integration with existing GiannaState** system\n",
    "‚úÖ **Tool interface** for AI agents and workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## üîß Setup and Dependencies\n",
    "\n",
    "Let's start by checking available dependencies and setting up the memory system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependency-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for optional dependencies\n",
    "dependencies_status = {\n",
    "    \"numpy\": False,\n",
    "    \"sentence_transformers\": False,\n",
    "    \"transformers\": False,\n",
    "    \"torch\": False,\n",
    "    \"faiss\": False,\n",
    "    \"chromadb\": False,\n",
    "    \"openai\": False\n",
    "}\n",
    "\n",
    "for package in dependencies_status:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        dependencies_status[package] = True\n",
    "        print(f\"‚úÖ {package} is available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package} is not available\")\n",
    "\n",
    "print(\"\\nüìä Dependency Status Summary:\")\n",
    "available = sum(dependencies_status.values())\n",
    "total = len(dependencies_status)\n",
    "print(f\"Available: {available}/{total} ({(available/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Add gianna to path if needed\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "# Import Gianna components\n",
    "try:\n",
    "    from gianna.memory import SemanticMemory, MemoryConfig\n",
    "    from gianna.memory.embeddings import EmbeddingProvider, get_available_providers\n",
    "    from gianna.memory.vectorstore import VectorStoreProvider, get_available_vector_stores\n",
    "    from gianna.memory.state_integration import MemoryIntegratedStateManager\n",
    "    from gianna.tools.memory_tools import create_semantic_memory_tool, get_available_memory_tools\n",
    "    \n",
    "    GIANNA_MEMORY_AVAILABLE = True\n",
    "    print(\"‚úÖ Gianna semantic memory system imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import Gianna memory system: {e}\")\n",
    "    GIANNA_MEMORY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-demo-section",
   "metadata": {},
   "source": [
    "## üöÄ Basic Memory System Demo\n",
    "\n",
    "Let's start with a basic demonstration of storing and retrieving conversations using semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GIANNA_MEMORY_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è Skipping demos - Gianna memory system not available\")\n",
    "else:\n",
    "    # Check available providers\n",
    "    available_embeddings = get_available_providers()\n",
    "    available_vectorstores = get_available_vector_stores()\n",
    "    available_tools = get_available_memory_tools()\n",
    "    \n",
    "    print(\"üîç Available Providers:\")\n",
    "    print(f\"  Embeddings: {[p.value for p in available_embeddings]}\")\n",
    "    print(f\"  Vector Stores: {[p.value for p in available_vectorstores]}\")\n",
    "    print(f\"  Memory Tools: {available_tools}\")\n",
    "    \n",
    "    # Create configuration with best available options\n",
    "    if available_embeddings:\n",
    "        embedding_provider = available_embeddings[0]  # Use first available\n",
    "    else:\n",
    "        print(\"‚ùå No embedding providers available\")\n",
    "        embedding_provider = None\n",
    "    \n",
    "    if available_vectorstores:\n",
    "        vectorstore_provider = available_vectorstores[0] if VectorStoreProvider.CHROMA in available_vectorstores else available_vectorstores[0]\n",
    "    else:\n",
    "        vectorstore_provider = VectorStoreProvider.IN_MEMORY  # Fallback\n",
    "    \n",
    "    print(f\"\\nüîß Using: {embedding_provider.value if embedding_provider else 'None'} + {vectorstore_provider.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GIANNA_MEMORY_AVAILABLE and embedding_provider:\n",
    "    # Create memory configuration\n",
    "    config = MemoryConfig(\n",
    "        embedding_provider=embedding_provider,\n",
    "        vectorstore_provider=vectorstore_provider,\n",
    "        similarity_threshold=0.7,\n",
    "        max_search_results=5,\n",
    "        enable_clustering=True,\n",
    "        cluster_similarity_threshold=0.85\n",
    "    )\n",
    "    \n",
    "    # Initialize semantic memory\n",
    "    try:\n",
    "        memory = SemanticMemory(config)\n",
    "        print(\"‚úÖ Semantic memory initialized successfully\")\n",
    "        print(f\"   Configuration: {config.embedding_provider.value} + {config.vectorstore_provider.value}\")\n",
    "        print(f\"   Similarity threshold: {config.similarity_threshold}\")\n",
    "        print(f\"   Clustering enabled: {config.enable_clustering}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize semantic memory: {e}\")\n",
    "        memory = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping memory initialization - requirements not met\")\n",
    "    memory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "store-interactions-section",
   "metadata": {},
   "source": [
    "### üìù Storing Sample Interactions\n",
    "\n",
    "Let's store some diverse interactions to demonstrate the memory system capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-interactions",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    # Sample interactions covering different topics\n",
    "    sample_interactions = [\n",
    "        # Programming and AI cluster\n",
    "        {\n",
    "            \"user_input\": \"How do I implement a neural network in Python?\",\n",
    "            \"assistant_response\": \"You can implement a neural network in Python using frameworks like TensorFlow or PyTorch. Start with defining layers, activation functions, and training loops.\",\n",
    "            \"context\": \"AI/ML programming discussion\",\n",
    "            \"type\": \"programming\",\n",
    "            \"topic\": \"neural_networks\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What's the difference between supervised and unsupervised learning?\",\n",
    "            \"assistant_response\": \"Supervised learning uses labeled training data to learn input-output mappings, while unsupervised learning finds patterns in data without labels.\",\n",
    "            \"context\": \"Machine learning concepts\",\n",
    "            \"type\": \"education\", \n",
    "            \"topic\": \"machine_learning\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"Can you help me debug this Python code?\",\n",
    "            \"assistant_response\": \"I'd be happy to help debug your Python code! Please share the code and describe the error you're encountering.\",\n",
    "            \"context\": \"Programming assistance\",\n",
    "            \"type\": \"programming\",\n",
    "            \"topic\": \"debugging\"\n",
    "        },\n",
    "        \n",
    "        # Web development cluster\n",
    "        {\n",
    "            \"user_input\": \"How do I create a REST API with Flask?\",\n",
    "            \"assistant_response\": \"To create a REST API with Flask: 1) Install Flask, 2) Create app instance, 3) Define routes with decorators, 4) Handle HTTP methods, 5) Return JSON responses.\",\n",
    "            \"context\": \"Web development tutorial\",\n",
    "            \"type\": \"programming\",\n",
    "            \"topic\": \"web_development\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What are the best practices for API design?\",\n",
    "            \"assistant_response\": \"API design best practices include: use RESTful conventions, proper HTTP status codes, consistent naming, versioning, authentication, rate limiting, and comprehensive documentation.\",\n",
    "            \"context\": \"Software architecture discussion\",\n",
    "            \"type\": \"architecture\",\n",
    "            \"topic\": \"api_design\"\n",
    "        },\n",
    "        \n",
    "        # Data science cluster\n",
    "        {\n",
    "            \"user_input\": \"How do I clean messy data in pandas?\",\n",
    "            \"assistant_response\": \"Data cleaning in pandas involves: handling missing values with dropna()/fillna(), removing duplicates with drop_duplicates(), data type conversion with astype(), and outlier detection.\",\n",
    "            \"context\": \"Data preprocessing tutorial\",\n",
    "            \"type\": \"data_science\",\n",
    "            \"topic\": \"data_cleaning\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What's the best way to visualize time series data?\",\n",
    "            \"assistant_response\": \"For time series visualization, use line plots for trends, seasonal decomposition plots for patterns, and interactive plots with plotly for exploration. Consider matplotlib, seaborn, or plotly.\",\n",
    "            \"context\": \"Data visualization guidance\",\n",
    "            \"type\": \"data_science\",\n",
    "            \"topic\": \"visualization\"\n",
    "        },\n",
    "        \n",
    "        # General assistance cluster\n",
    "        {\n",
    "            \"user_input\": \"Can you recommend some good tech books?\",\n",
    "            \"assistant_response\": \"Great tech books include: 'Clean Code' by Robert Martin, 'The Pragmatic Programmer' by Hunt & Thomas, 'Designing Data-Intensive Applications' by Martin Kleppmann.\",\n",
    "            \"context\": \"Book recommendations\",\n",
    "            \"type\": \"recommendations\",\n",
    "            \"topic\": \"books\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What's the weather like today?\",\n",
    "            \"assistant_response\": \"I don't have access to real-time weather data. Please check a weather app or website like weather.com for current conditions in your area.\",\n",
    "            \"context\": \"General inquiry\",\n",
    "            \"type\": \"general\",\n",
    "            \"topic\": \"weather\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìù Storing {len(sample_interactions)} sample interactions...\")\n",
    "    session_id = \"demo_session_001\"\n",
    "    interaction_ids = []\n",
    "    \n",
    "    for i, interaction in enumerate(sample_interactions, 1):\n",
    "        interaction_id = memory.store_interaction(\n",
    "            session_id=session_id,\n",
    "            user_input=interaction[\"user_input\"],\n",
    "            assistant_response=interaction[\"assistant_response\"],\n",
    "            context=interaction[\"context\"],\n",
    "            interaction_type=interaction[\"type\"],\n",
    "            metadata={\n",
    "                \"topic\": interaction[\"topic\"],\n",
    "                \"demo_sequence\": i,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if interaction_id:\n",
    "            interaction_ids.append(interaction_id)\n",
    "            print(f\"  ‚úÖ [{i:2d}] {interaction['type']:12} | {interaction['user_input'][:50]}...\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå [{i:2d}] Failed to store interaction\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully stored {len(interaction_ids)} interactions\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping interaction storage - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semantic-search-section",
   "metadata": {},
   "source": [
    "### üîç Semantic Similarity Search\n",
    "\n",
    "Now let's demonstrate the power of semantic search - finding related conversations even with different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "semantic-search-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üîç Semantic Search Demonstrations\\n\")\n",
    "    \n",
    "    # Test queries with semantic similarity\n",
    "    search_queries = [\n",
    "        {\n",
    "            \"query\": \"machine learning and artificial intelligence\",\n",
    "            \"description\": \"Should find neural networks, ML concepts\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"building web applications and servers\", \n",
    "            \"description\": \"Should find Flask API, web development\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"data analysis and processing techniques\",\n",
    "            \"description\": \"Should find pandas, data visualization\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"code issues and troubleshooting\",\n",
    "            \"description\": \"Should find debugging assistance\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"learning resources and educational materials\",\n",
    "            \"description\": \"Should find book recommendations\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, search in enumerate(search_queries, 1):\n",
    "        print(f\"üîé Search {i}: '{search['query']}'\")\n",
    "        print(f\"   Expected: {search['description']}\")\n",
    "        \n",
    "        try:\n",
    "            results = memory.search_similar_interactions(\n",
    "                query=search[\"query\"],\n",
    "                session_id=session_id,\n",
    "                max_results=3,\n",
    "                similarity_threshold=0.5  # Lower threshold for demo\n",
    "            )\n",
    "            \n",
    "            if results:\n",
    "                print(f\"   Found {len(results)} similar interactions:\")\n",
    "                for j, result in enumerate(results, 1):\n",
    "                    topic = result.metadata.get('topic', 'unknown')\n",
    "                    print(f\"     {j}. [{topic:15}] {result.user_input[:60]}...\")\n",
    "            else:\n",
    "                print(\"     No similar interactions found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Search error: {e}\")\n",
    "        \n",
    "        print()  # Empty line for readability\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping semantic search demo - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern-detection-section",
   "metadata": {},
   "source": [
    "### üß© Pattern Detection and Analysis\n",
    "\n",
    "The system can automatically detect patterns in user interactions, including topics, preferences, and conversation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üß© Pattern Detection and Analysis\\n\")\n",
    "    \n",
    "    try:\n",
    "        patterns = memory.detect_patterns(session_id)\n",
    "        \n",
    "        print(\"üìä Interaction Statistics:\")\n",
    "        print(f\"   Total interactions: {patterns.get('total_interactions', 0)}\")\n",
    "        \n",
    "        if 'time_span' in patterns:\n",
    "            time_span = patterns['time_span']\n",
    "            print(f\"   Session duration: {time_span.get('duration_minutes', 0)} minutes\")\n",
    "        \n",
    "        print(\"\\nüè∑Ô∏è Interaction Types:\")\n",
    "        for interaction_type, count in patterns.get('interaction_types', {}).items():\n",
    "            print(f\"   {interaction_type:15}: {count} interactions\")\n",
    "        \n",
    "        print(\"\\nüí≠ Common Themes:\")\n",
    "        themes = patterns.get('common_themes', [])[:8]  # Show top 8\n",
    "        if themes:\n",
    "            for theme in themes:\n",
    "                print(f\"   ‚Ä¢ {theme}\")\n",
    "        else:\n",
    "            print(\"   No common themes detected\")\n",
    "        \n",
    "        print(\"\\nüë§ User Preferences:\")\n",
    "        preferences = patterns.get('user_preferences', {})\n",
    "        for key, value in preferences.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"   {key:25}: {value:.1f}\")\n",
    "            else:\n",
    "                print(f\"   {key:25}: {value}\")\n",
    "        \n",
    "        print(\"\\nüó£Ô∏è Conversation Patterns:\")\n",
    "        conv_patterns = patterns.get('conversation_patterns', {})\n",
    "        for key, value in conv_patterns.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {key:25}: {value:.3f}\")\n",
    "            else:\n",
    "                print(f\"   {key:25}: {value}\")\n",
    "        \n",
    "        print(\"\\nüéØ Detected Clusters:\")\n",
    "        clusters = patterns.get('clusters', {})\n",
    "        if clusters:\n",
    "            for i, (cluster_id, interaction_ids) in enumerate(clusters.items(), 1):\n",
    "                print(f\"   Cluster {i}: {len(interaction_ids)} interactions (ID: {cluster_id[:8]}...)\")\n",
    "        else:\n",
    "            print(\"   No clusters detected (need more similar interactions)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pattern detection error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pattern analysis - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-summary-section",
   "metadata": {},
   "source": [
    "### üìã Context Summarization\n",
    "\n",
    "The system can generate intelligent summaries of conversation context, useful for maintaining context across long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "context-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üìã Context Summarization Demo\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get context summary for different interaction counts\n",
    "        summary_configs = [\n",
    "            {\"max_interactions\": 3, \"description\": \"Recent 3 interactions\"},\n",
    "            {\"max_interactions\": 5, \"description\": \"Recent 5 interactions\"},\n",
    "            {\"max_interactions\": None, \"description\": \"All interactions (default window)\"}\n",
    "        ]\n",
    "        \n",
    "        for config in summary_configs:\n",
    "            print(f\"üìÑ {config['description']}:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            summary = memory.get_context_summary(\n",
    "                session_id=session_id,\n",
    "                max_interactions=config['max_interactions']\n",
    "            )\n",
    "            \n",
    "            # Format the summary nicely\n",
    "            lines = summary.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "            \n",
    "            print()  # Empty line between summaries\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Context summary error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping context summarization - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-stats-section",
   "metadata": {},
   "source": [
    "### üìà Memory System Statistics\n",
    "\n",
    "Let's examine the memory system performance and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üìà Memory System Statistics\\n\")\n",
    "    \n",
    "    try:\n",
    "        stats = memory.get_memory_stats()\n",
    "        \n",
    "        print(\"üèóÔ∏è System Configuration:\")\n",
    "        print(f\"   Embedding provider:        {stats.get('embedding_provider')}\")\n",
    "        print(f\"   Vector store provider:      {stats.get('vector_store_provider')}\")\n",
    "        print(f\"   Embedding dimension:        {stats.get('embedding_dimension')}\")\n",
    "        print(f\"   Similarity threshold:       {stats.get('average_similarity_threshold')}\")\n",
    "        \n",
    "        print(\"\\nüìä Storage Statistics:\")\n",
    "        print(f\"   Total interactions:         {stats.get('total_interactions')}\")\n",
    "        print(f\"   Cached interactions:        {stats.get('cached_interactions')}\")\n",
    "        print(f\"   Total summaries:            {stats.get('total_summaries')}\")\n",
    "        \n",
    "        print(\"\\n‚ö° Performance Metrics:\")\n",
    "        print(f\"   Cache hit rate:             {stats.get('cache_hit_rate', 0):.1%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Statistics error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping statistics - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-features-section",
   "metadata": {},
   "source": [
    "## üöÄ Advanced Features Demo\n",
    "\n",
    "Let's explore some advanced features including filtered search, global search across sessions, and memory cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üîç Advanced Search Features\\n\")\n",
    "    \n",
    "    # Store interactions in a second session for global search demo\n",
    "    second_session_id = \"demo_session_002\"\n",
    "    \n",
    "    # Add some interactions to second session\n",
    "    additional_interactions = [\n",
    "        {\n",
    "            \"user_input\": \"How do I deploy a machine learning model to production?\",\n",
    "            \"assistant_response\": \"ML model deployment involves containerization with Docker, API creation, monitoring, scaling, and CI/CD pipelines. Consider platforms like AWS SageMaker or Kubernetes.\",\n",
    "            \"context\": \"MLOps discussion\",\n",
    "            \"type\": \"deployment\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What are the security considerations for web APIs?\",\n",
    "            \"assistant_response\": \"Web API security includes authentication (JWT/OAuth), authorization, input validation, rate limiting, HTTPS encryption, and protection against common attacks like CSRF and injection.\",\n",
    "            \"context\": \"Security best practices\",\n",
    "            \"type\": \"security\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìù Adding {len(additional_interactions)} interactions to second session...\")\n",
    "    for interaction in additional_interactions:\n",
    "        memory.store_interaction(\n",
    "            session_id=second_session_id,\n",
    "            user_input=interaction[\"user_input\"],\n",
    "            assistant_response=interaction[\"assistant_response\"],\n",
    "            context=interaction[\"context\"],\n",
    "            interaction_type=interaction[\"type\"],\n",
    "            metadata={\"session\": \"second\"}\n",
    "        )\n",
    "    \n",
    "    print(\"\\nüîé Filtered Search by Interaction Type:\")\n",
    "    programming_results = memory.search_similar_interactions(\n",
    "        query=\"coding and development\",\n",
    "        session_id=session_id,\n",
    "        interaction_type=\"programming\",\n",
    "        max_results=3\n",
    "    )\n",
    "    \n",
    "    print(f\"   Found {len(programming_results)} programming-related interactions:\")\n",
    "    for result in programming_results:\n",
    "        print(f\"     ‚Ä¢ {result.user_input[:60]}...\")\n",
    "    \n",
    "    print(\"\\nüåç Global Search Across All Sessions:\")\n",
    "    global_results = memory.search_similar_interactions(\n",
    "        query=\"machine learning and AI models\",\n",
    "        session_id=None,  # Search all sessions\n",
    "        max_results=5\n",
    "    )\n",
    "    \n",
    "    print(f\"   Found {len(global_results)} interactions across all sessions:\")\n",
    "    for result in global_results:\n",
    "        session_label = \"Session 1\" if result.session_id == session_id else \"Session 2\"\n",
    "        print(f\"     ‚Ä¢ [{session_label}] {result.user_input[:50]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping advanced search features - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-integration-section",
   "metadata": {},
   "source": [
    "## üîß Tool Integration Demo\n",
    "\n",
    "The semantic memory system integrates with Gianna's tool system for use in AI agents and workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory and \"semantic_memory\" in get_available_memory_tools():\n",
    "    print(\"üîß Tool Integration Demo\\n\")\n",
    "    \n",
    "    # Create semantic memory tool\n",
    "    try:\n",
    "        memory_tool = create_semantic_memory_tool(config)\n",
    "        \n",
    "        if memory_tool:\n",
    "            print(\"‚úÖ Semantic memory tool created successfully\")\n",
    "            print(f\"   Tool name: {memory_tool.name}\")\n",
    "            print(f\"   Description: {memory_tool.description[:80]}...\")\n",
    "            \n",
    "            # Test tool operations\n",
    "            print(\"\\nüß™ Testing Tool Operations:\")\n",
    "            \n",
    "            # Test search operation\n",
    "            search_request = {\n",
    "                \"action\": \"search\",\n",
    "                \"query\": \"Python programming and development\",\n",
    "                \"session_id\": session_id,\n",
    "                \"max_results\": 2,\n",
    "                \"similarity_threshold\": 0.6\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüîç Search Request: {search_request['query']}\")\n",
    "            search_result = memory_tool._run(json.dumps(search_request))\n",
    "            search_data = json.loads(search_result)\n",
    "            \n",
    "            if search_data.get('success'):\n",
    "                results = search_data.get('results', [])\n",
    "                print(f\"   ‚úÖ Found {len(results)} results\")\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"     {i}. {result['user_input'][:50]}...\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Search failed: {search_data.get('error')}\")\n",
    "            \n",
    "            # Test pattern detection\n",
    "            pattern_request = {\n",
    "                \"action\": \"patterns\",\n",
    "                \"session_id\": session_id\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüß© Pattern Detection Request\")\n",
    "            pattern_result = memory_tool._run(json.dumps(pattern_request))\n",
    "            pattern_data = json.loads(pattern_result)\n",
    "            \n",
    "            if pattern_data.get('success'):\n",
    "                patterns = pattern_data.get('patterns', {})\n",
    "                print(f\"   ‚úÖ Pattern analysis completed\")\n",
    "                print(f\"   Total interactions: {patterns.get('total_interactions', 0)}\")\n",
    "                print(f\"   Interaction types: {list(patterns.get('interaction_types', {}).keys())}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Pattern detection failed: {pattern_data.get('error')}\")\n",
    "            \n",
    "            # Test statistics\n",
    "            stats_request = {\"action\": \"stats\"}\n",
    "            \n",
    "            print(f\"\\nüìà Statistics Request\")\n",
    "            stats_result = memory_tool._run(json.dumps(stats_request))\n",
    "            stats_data = json.loads(stats_result)\n",
    "            \n",
    "            if stats_data.get('success'):\n",
    "                stats = stats_data.get('stats', {})\n",
    "                print(f\"   ‚úÖ Statistics retrieved\")\n",
    "                print(f\"   Provider: {stats.get('embedding_provider')} + {stats.get('vector_store_provider')}\")\n",
    "                print(f\"   Total interactions: {stats.get('total_interactions')}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Statistics failed: {stats_data.get('error')}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Failed to create semantic memory tool\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Tool integration error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping tool integration demo - semantic memory tool not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-integration-section",
   "metadata": {},
   "source": [
    "## üîÑ State Manager Integration\n",
    "\n",
    "The semantic memory system integrates seamlessly with Gianna's existing state management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GIANNA_MEMORY_AVAILABLE:\n",
    "    print(\"üîÑ State Manager Integration Demo\\n\")\n",
    "    \n",
    "    try:\n",
    "        from gianna.core.state import create_initial_state\n",
    "        \n",
    "        # Create memory-integrated state manager\n",
    "        integrated_manager = MemoryIntegratedStateManager(\n",
    "            db_path=\"demo_gianna_state.db\",\n",
    "            memory_config=config\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Memory-integrated state manager created\")\n",
    "        \n",
    "        # Create a demo state with conversation\n",
    "        test_session_id = \"state_integration_test\"\n",
    "        state = create_initial_state(test_session_id)\n",
    "        \n",
    "        # Add some conversation messages\n",
    "        state[\"conversation\"].messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How do I optimize database queries?\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": \"Database query optimization involves indexing, query analysis, proper joins, avoiding N+1 problems, and using database-specific optimization features.\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüíæ Saving state with automatic semantic memory integration...\")\n",
    "        integrated_manager.save_state(test_session_id, state)\n",
    "        print(\"   ‚úÖ State saved (conversations automatically stored in semantic memory)\")\n",
    "        \n",
    "        # Load state with enhanced context\n",
    "        print(\"\\nüìñ Loading state with enhanced semantic context...\")\n",
    "        loaded_state = integrated_manager.load_state(test_session_id)\n",
    "        \n",
    "        if loaded_state:\n",
    "            print(\"   ‚úÖ State loaded successfully\")\n",
    "            print(f\"   Messages: {len(loaded_state['conversation'].messages)}\")\n",
    "            if loaded_state[\"conversation\"].context_summary:\n",
    "                print(f\"   Enhanced context available: {len(loaded_state['conversation'].context_summary)} characters\")\n",
    "        \n",
    "        # Test semantic search through integrated manager\n",
    "        print(\"\\nüîç Testing semantic search through integrated manager...\")\n",
    "        similar_convs = integrated_manager.search_similar_conversations(\n",
    "            query=\"database and SQL optimization\",\n",
    "            session_id=test_session_id,\n",
    "            max_results=3\n",
    "        )\n",
    "        \n",
    "        print(f\"   Found {len(similar_convs)} similar conversations\")\n",
    "        for conv in similar_convs:\n",
    "            print(f\"     ‚Ä¢ {conv.user_input[:50]}...\")\n",
    "        \n",
    "        # Get comprehensive statistics\n",
    "        print(\"\\nüìä Comprehensive Memory Statistics:\")\n",
    "        comprehensive_stats = integrated_manager.get_memory_statistics()\n",
    "        for key, value in comprehensive_stats.items():\n",
    "            print(f\"   {key:25}: {value}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå State integration error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping state integration demo - Gianna memory system not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-section",
   "metadata": {},
   "source": [
    "## ‚ö° Performance and Scalability\n",
    "\n",
    "Let's test the system's performance with a larger dataset and examine scalability characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    import time\n",
    "    \n",
    "    print(\"‚ö° Performance and Scalability Testing\\n\")\n",
    "    \n",
    "    # Performance test with batch operations\n",
    "    performance_session = \"performance_test_session\"\n",
    "    \n",
    "    # Generate more test interactions for performance testing\n",
    "    performance_interactions = [\n",
    "        {\n",
    "            \"user_input\": f\"What is the best approach for {topic}?\",\n",
    "            \"assistant_response\": f\"For {topic}, I recommend following best practices including proper planning, testing, and documentation.\",\n",
    "            \"context\": f\"Discussion about {topic}\",\n",
    "            \"type\": \"question\",\n",
    "            \"topic\": topic\n",
    "        }\n",
    "        for topic in [\n",
    "            \"microservices architecture\", \"containerization with Docker\", \n",
    "            \"continuous integration\", \"test-driven development\",\n",
    "            \"database design patterns\", \"API rate limiting\",\n",
    "            \"caching strategies\", \"load balancing\",\n",
    "            \"monitoring and observability\", \"security hardening\",\n",
    "            \"performance optimization\", \"code review processes\",\n",
    "            \"error handling patterns\", \"logging best practices\",\n",
    "            \"configuration management\"\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    print(f\"üèÉ Performance Test: Storing {len(performance_interactions)} interactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    stored_count = 0\n",
    "    for interaction in performance_interactions:\n",
    "        interaction_id = memory.store_interaction(\n",
    "            session_id=performance_session,\n",
    "            user_input=interaction[\"user_input\"],\n",
    "            assistant_response=interaction[\"assistant_response\"],\n",
    "            context=interaction[\"context\"],\n",
    "            interaction_type=interaction[\"type\"],\n",
    "            metadata={\"topic\": interaction[\"topic\"], \"batch\": \"performance_test\"}\n",
    "        )\n",
    "        if interaction_id:\n",
    "            stored_count += 1\n",
    "    \n",
    "    storage_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ‚úÖ Stored {stored_count} interactions in {storage_time:.2f} seconds\")\n",
    "    print(f\"   Average storage time: {(storage_time/stored_count)*1000:.1f} ms per interaction\")\n",
    "    \n",
    "    # Test search performance\n",
    "    search_queries = [\n",
    "        \"software architecture and design patterns\",\n",
    "        \"DevOps and deployment strategies\", \n",
    "        \"performance monitoring and optimization\",\n",
    "        \"security and best practices\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüîç Search Performance Test: {len(search_queries)} queries...\")\n",
    "    total_search_time = 0\n",
    "    total_results = 0\n",
    "    \n",
    "    for query in search_queries:\n",
    "        start_time = time.time()\n",
    "        results = memory.search_similar_interactions(\n",
    "            query=query,\n",
    "            session_id=performance_session,\n",
    "            max_results=5\n",
    "        )\n",
    "        search_time = time.time() - start_time\n",
    "        total_search_time += search_time\n",
    "        total_results += len(results)\n",
    "        \n",
    "        print(f\"   Query: '{query[:40]}...' ‚Üí {len(results)} results in {search_time*1000:.1f} ms\")\n",
    "    \n",
    "    print(f\"\\nüìä Search Performance Summary:\")\n",
    "    print(f\"   Average search time: {(total_search_time/len(search_queries))*1000:.1f} ms\")\n",
    "    print(f\"   Average results per query: {total_results/len(search_queries):.1f}\")\n",
    "    \n",
    "    # Final system statistics\n",
    "    print(\"\\nüìà Final System Statistics:\")\n",
    "    final_stats = memory.get_memory_stats()\n",
    "    print(f\"   Total interactions: {final_stats.get('total_interactions')}\")\n",
    "    print(f\"   Cached interactions: {final_stats.get('cached_interactions')}\")\n",
    "    print(f\"   Cache efficiency: {final_stats.get('cache_hit_rate', 0):.1%}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping performance testing - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## üßπ Memory Cleanup and Maintenance\n",
    "\n",
    "The system includes automatic cleanup capabilities to manage storage over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory:\n",
    "    print(\"üßπ Memory Cleanup and Maintenance\\n\")\n",
    "    \n",
    "    # Show current statistics before cleanup\n",
    "    print(\"üìä Before Cleanup:\")\n",
    "    stats_before = memory.get_memory_stats()\n",
    "    print(f\"   Total interactions: {stats_before.get('total_interactions')}\")\n",
    "    print(f\"   Cached interactions: {stats_before.get('cached_interactions')}\")\n",
    "    \n",
    "    # Simulate cleanup (we'll use a very small age threshold for demo)\n",
    "    print(\"\\nüóëÔ∏è Simulating cleanup of very old interactions...\")\n",
    "    print(\"   Note: Using 0 days threshold for demo - in practice use 30+ days\")\n",
    "    \n",
    "    try:\n",
    "        # In real usage, you'd use a reasonable threshold like 30 days\n",
    "        # For demo purposes, we'll show what the cleanup would report\n",
    "        cleanup_count = memory.cleanup_old_interactions(max_age_days=0)\n",
    "        print(f\"   Cleaned up {cleanup_count} interactions\")\n",
    "        \n",
    "        if cleanup_count == 0:\n",
    "            print(\"   (No interactions old enough to clean up in this demo)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cleanup error: {e}\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    print(\"\\nüìä After Cleanup:\")\n",
    "    stats_after = memory.get_memory_stats()\n",
    "    print(f\"   Total interactions: {stats_after.get('total_interactions')}\")\n",
    "    print(f\"   Cached interactions: {stats_after.get('cached_interactions')}\")\n",
    "    \n",
    "    print(\"\\n‚ú® Cleanup Features:\")\n",
    "    print(\"   ‚Ä¢ Configurable age thresholds\")\n",
    "    print(\"   ‚Ä¢ Preserves recent interactions\")\n",
    "    print(\"   ‚Ä¢ Maintains vector store consistency\")\n",
    "    print(\"   ‚Ä¢ Can be scheduled for automatic maintenance\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping cleanup demo - memory system not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## üéâ Demo Summary and Conclusions\n",
    "\n",
    "Let's summarize what we've demonstrated and the capabilities of the Gianna Semantic Memory System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Gianna Semantic Memory System - Demo Summary\")\nprint(\"=\" * 60)\n\nprint(\"\\n‚úÖ Successfully Demonstrated:\")\n\ndemonstrated_features = [\n    \"üß† Semantic embedding and storage of conversations\",\n    \"üîç Intelligent similarity search with configurable thresholds\", \n    \"üéØ Automatic clustering of related interactions\",\n    \"üìã Context summarization with intelligent aggregation\",\n    \"üß© Pattern detection and user preference analysis\",\n    \"üîß Tool integration for AI agents and workflows\",\n    \"üîÑ Seamless integration with existing GiannaState system\",\n    \"‚ö° Performance optimization with caching strategies\",\n    \"üßπ Automatic cleanup and maintenance capabilities\",\n    \"üõ°Ô∏è Fallback strategies for missing dependencies\"\n]\n\nfor feature in demonstrated_features:\n    print(f\"   {feature}\")\n\nprint(\"\\nüèóÔ∏è System Architecture Highlights:\")\narchitecture_points = [\n    \"Multiple embedding providers (OpenAI, SentenceTransformers, HuggingFace)\",\n    \"Multiple vector stores (ChromaDB, FAISS, In-Memory)\",\n    \"Intelligent provider selection with automatic fallbacks\",\n    \"Persistent storage with SQLite integration\",\n    \"Configurable similarity thresholds and search parameters\",\n    \"Tool interface compatible with LangChain/LangGraph workflows\",\n    \"Extensible architecture for adding new providers\"\n]\n\nfor point in architecture_points:\n    print(f\"   ‚Ä¢ {point}\")\n\nif GIANNA_MEMORY_AVAILABLE:\n    print(\"\\nüìä Final System Status:\")\n    if memory:\n        final_stats = memory.get_memory_stats()\n        print(f\"   Configuration: {final_stats.get('embedding_provider')} + {final_stats.get('vector_store_provider')}\")\n        print(f\"   Total interactions processed: {final_stats.get('total_interactions')}\")\n        print(f\"   Embedding dimension: {final_stats.get('embedding_dimension')}\")\n        print(f\"   Cache efficiency: {final_stats.get('cache_hit_rate', 0):.1%}\")\n    \n    available_embeddings = get_available_providers()\n    available_vectorstores = get_available_vector_stores()\n    print(f\"   Available embedding providers: {len(available_embeddings)}\")\n    print(f\"   Available vector stores: {len(available_vectorstores)}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Note: Some features were not demonstrated due to missing dependencies\")\n    print(\"   Install optional packages for full functionality:\")\n    print(\"   ‚Ä¢ pip install sentence-transformers (local embeddings)\")\n    print(\"   ‚Ä¢ pip install transformers torch (HuggingFace embeddings)\")\n    print(\"   ‚Ä¢ pip install faiss-cpu (FAISS vector store)\")\n    print(\"   ‚Ä¢ pip install chromadb (ChromaDB vector store)\")\n\nprint(\"\\nüöÄ Next Steps:\")\nnext_steps = [\n    \"Integrate with your Gianna workflows using the tool interface\",\n    \"Configure embedding and vector store providers for your use case\",\n    \"Set up automatic cleanup schedules for production environments\",\n    \"Customize similarity thresholds based on your domain\",\n    \"Extend with custom embedding providers or vector stores as needed\"\n]\n\nfor step in next_steps:\n    print(f\"   1. {step}\")\n    next_steps[0] = \"2.\"  # Simple way to increment\n\nprint(\"\\n‚ú® The Gianna Semantic Memory System is ready for production use!\")\nprint(\"   Built with flexibility, performance, and reliability in mind.\")\nprint(\"   Enjoy enhanced conversational AI with long-term memory! üéØ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
